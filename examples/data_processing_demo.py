"""
Data Processing Demo

ChemForgeãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
ChEMBLãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»åˆ†å­ç‰¹å¾´é‡æŠ½å‡ºãƒ»å‰å‡¦ç†ã®çµ±åˆä¾‹
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List
import time
import logging

from chemforge.data.chembl_loader import ChEMBLLoader
from chemforge.data.molecular_features import MolecularFeatures
from chemforge.data.rdkit_descriptors import RDKitDescriptors
from chemforge.data.data_preprocessor import DataPreprocessor

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_synthetic_data(num_compounds: int = 1000) -> pd.DataFrame:
    """
    åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
    
    Args:
        num_compounds: åŒ–åˆç‰©æ•°
    
    Returns:
        åˆæˆãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    logger.info(f"Creating synthetic data with {num_compounds} compounds")
    
    # åˆæˆSMILES
    smiles_list = [
        "CCO", "CCN", "CCC", "CCCC", "CCCCC",
        "C1=CC=CC=C1", "C1=CC=CC=C1O", "C1=CC=CC=C1N",
        "C1=CC=CC=C1C", "C1=CC=CC=C1CC",
        "C1=CC=CC=C1CCC", "C1=CC=CC=C1CCCC",
        "C1=CC=CC=C1CCCCC", "C1=CC=CC=C1CCCCCC",
        "C1=CC=CC=C1CCCCCCC", "C1=CC=CC=C1CCCCCCCC"
    ] * (num_compounds // 16 + 1)
    smiles_list = smiles_list[:num_compounds]
    
    # åˆæˆpIC50å€¤
    np.random.seed(42)
    data = {
        'molecule_chembl_id': [f'CHEMBL{i:06d}' for i in range(num_compounds)],
        'canonical_smiles': smiles_list,
        '5HT2A_pIC50': np.random.normal(7.5, 1.0, num_compounds),
        'D1_pIC50': np.random.normal(7.0, 1.0, num_compounds),
        'CB1_pIC50': np.random.normal(6.5, 1.0, num_compounds),
        'MOR_pIC50': np.random.normal(7.2, 1.0, num_compounds)
    }
    
    df = pd.DataFrame(data)
    
    # pIC50å€¤ã‚’6-10ã®ç¯„å›²ã«ã‚¯ãƒ©ãƒ³ãƒ—
    for col in ['5HT2A_pIC50', 'D1_pIC50', 'CB1_pIC50', 'MOR_pIC50']:
        df[col] = np.clip(df[col], 6.0, 10.0)
    
    logger.info(f"Created synthetic dataset with {len(df)} compounds")
    return df


def demonstrate_chembl_loader():
    """ChEMBLãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\n" + "="*60)
    print("ğŸ”¬ ChEMBL Data Loader Demo")
    print("="*60)
    
    # ChEMBLãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–
    loader = ChEMBLLoader(cache_dir="./data/cache")
    
    # åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆå®Ÿéš›ã®ChEMBL APIã®ä»£ã‚ã‚Šï¼‰
    synthetic_data = create_synthetic_data(500)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¦ç´„ã‚’è¡¨ç¤º
    summary = loader.get_dataset_summary(synthetic_data)
    print(f"\nğŸ“Š Dataset Summary:")
    print(f"  Total compounds: {summary['total_compounds']}")
    print(f"  Targets: {summary['targets']}")
    print(f"  Missing values: {sum(summary['missing_values'].values())}")
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ¥çµ±è¨ˆ
    print(f"\nğŸ¯ Target Statistics:")
    for target, stats in summary['target_statistics'].items():
        print(f"  {target}:")
        print(f"    Count: {stats['count']}")
        print(f"    Mean: {stats['mean']:.3f}")
        print(f"    Std: {stats['std']:.3f}")
        print(f"    Range: {stats['min']:.3f} - {stats['max']:.3f}")
    
    return synthetic_data


def demonstrate_molecular_features():
    """åˆ†å­ç‰¹å¾´é‡æŠ½å‡ºã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\n" + "="*60)
    print("ğŸ§¬ Molecular Features Demo")
    print("="*60)
    
    # åˆ†å­ç‰¹å¾´é‡æŠ½å‡ºå™¨ã‚’åˆæœŸåŒ–
    features = MolecularFeatures()
    
    # ãƒ†ã‚¹ãƒˆåˆ†å­
    test_smiles = [
        "CCO",  # ã‚¨ã‚¿ãƒãƒ¼ãƒ«
        "C1=CC=CC=C1",  # ãƒ™ãƒ³ã‚¼ãƒ³
        "C1=CC=CC=C1O",  # ãƒ•ã‚§ãƒãƒ¼ãƒ«
        "C1=CC=CC=C1N",  # ã‚¢ãƒ‹ãƒªãƒ³
        "C1=CC=CC=C1C(=O)O"  # å®‰æ¯é¦™é…¸
    ]
    
    print(f"\nğŸ” Processing {len(test_smiles)} test molecules...")
    
    # 2Dè¨˜è¿°å­ã‚’æŠ½å‡º
    print("\nğŸ“‹ 2D Descriptors:")
    for smiles in test_smiles:
        descriptors = features.extract_2d_descriptors(smiles)
        print(f"  {smiles}:")
        print(f"    MolWt: {descriptors.get('MolWt', 0):.2f}")
        print(f"    LogP: {descriptors.get('LogP', 0):.2f}")
        print(f"    TPSA: {descriptors.get('TPSA', 0):.2f}")
        print(f"    NumRings: {descriptors.get('NumRings', 0)}")
    
    # ãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ—ãƒªãƒ³ãƒˆã‚’æŠ½å‡º
    print("\nğŸ”¢ Fingerprints:")
    for smiles in test_smiles:
        morgan_fp = features.extract_fingerprints(smiles, "morgan")
        rdkit_fp = features.extract_fingerprints(smiles, "rdkit")
        print(f"  {smiles}:")
        print(f"    Morgan FP: {morgan_fp.sum()} bits set")
        print(f"    RDKit FP: {rdkit_fp.sum()} bits set")
    
    # éª¨æ ¼ç‰¹å¾´é‡ã‚’æŠ½å‡º
    print("\nğŸ—ï¸ Scaffold Features:")
    for smiles in test_smiles:
        scaffold_features = features.extract_scaffold_features(smiles)
        print(f"  {smiles}:")
        print(f"    Scaffold: {scaffold_features.get('scaffold_smiles', 'N/A')}")
        print(f"    Scaffold atoms: {scaffold_features.get('scaffold_atoms', 0)}")
        print(f"    Scaffold rings: {scaffold_features.get('scaffold_rings', 0)}")
    
    return features


def demonstrate_rdkit_descriptors():
    """RDKitè¨˜è¿°å­ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\n" + "="*60)
    print("âš—ï¸ RDKit Descriptors Demo")
    print("="*60)
    
    # RDKitè¨˜è¿°å­æŠ½å‡ºå™¨ã‚’åˆæœŸåŒ–
    descriptors = RDKitDescriptors()
    
    # ãƒ†ã‚¹ãƒˆåˆ†å­
    test_smiles = [
        "CCO",  # ã‚¨ã‚¿ãƒãƒ¼ãƒ«
        "C1=CC=CC=C1",  # ãƒ™ãƒ³ã‚¼ãƒ³
        "C1=CC=CC=C1O",  # ãƒ•ã‚§ãƒãƒ¼ãƒ«
        "C1=CC=CC=C1N",  # ã‚¢ãƒ‹ãƒªãƒ³
        "C1=CC=CC=C1C(=O)O"  # å®‰æ¯é¦™é…¸
    ]
    
    print(f"\nğŸ” Processing {len(test_smiles)} test molecules...")
    
    # åŸºæœ¬è¨˜è¿°å­ã‚’è¨ˆç®—
    print("\nğŸ“Š Basic Descriptors:")
    for smiles in test_smiles:
        basic_desc = descriptors.calculate_basic_descriptors(
            descriptors._get_descriptor_functions()['MolWt'].__self__.MolFromSmiles(smiles)
        )
        print(f"  {smiles}:")
        print(f"    MolWt: {basic_desc.get('MolWt', 0):.2f}")
        print(f"    LogP: {basic_desc.get('LogP', 0):.2f}")
        print(f"    TPSA: {basic_desc.get('TPSA', 0):.2f}")
        print(f"    NumRings: {basic_desc.get('NumRings', 0)}")
    
    # Lipinski's Ruleã‚’è¨ˆç®—
    print("\nğŸ“ Lipinski's Rule of Five:")
    for smiles in test_smiles:
        lipinski = descriptors.calculate_lipinski_rule(
            descriptors._get_descriptor_functions()['MolWt'].__self__.MolFromSmiles(smiles)
        )
        print(f"  {smiles}:")
        print(f"    MolWt: {lipinski.get('MolWt', 0):.2f}")
        print(f"    LogP: {lipinski.get('LogP', 0):.2f}")
        print(f"    HBD: {lipinski.get('NumHDonors', 0)}")
        print(f"    HBA: {lipinski.get('NumHAcceptors', 0)}")
        print(f"    Violations: {lipinski.get('LipinskiViolations', 0)}")
        print(f"    Compliant: {lipinski.get('LipinskiCompliant', False)}")
    
    return descriptors


def demonstrate_data_preprocessing():
    """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\n" + "="*60)
    print("ğŸ”§ Data Preprocessing Demo")
    print("="*60)
    
    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å™¨ã‚’åˆæœŸåŒ–
    preprocessor = DataPreprocessor()
    
    # åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    synthetic_data = create_synthetic_data(1000)
    
    # å‰å‡¦ç†è¨­å®š
    preprocessing_config = {
        "clean_data": True,
        "handle_outliers": True,
        "impute_missing": True,
        "normalize_features": True,
        "min_pic50": 6.0,
        "max_pic50": 10.0,
        "outlier_method": "iqr",
        "impute_strategy": "median",
        "normalization_method": "standard"
    }
    
    # ç‰¹å¾´é‡åˆ—ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã‚’å®šç¾©
    feature_columns = ['MolWt', 'LogP', 'TPSA', 'NumRings']
    target_columns = ['5HT2A_pIC50', 'D1_pIC50', 'CB1_pIC50', 'MOR_pIC50']
    
    # åˆæˆç‰¹å¾´é‡ã‚’è¿½åŠ 
    np.random.seed(42)
    for col in feature_columns:
        synthetic_data[col] = np.random.normal(100, 20, len(synthetic_data))
    
    print(f"\nğŸ“Š Original Dataset:")
    print(f"  Size: {len(synthetic_data)}")
    print(f"  Features: {len(feature_columns)}")
    print(f"  Targets: {len(target_columns)}")
    
    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’å®Ÿè¡Œ
    processed_data, preprocessing_info = preprocessor.preprocess_dataset(
        synthetic_data,
        target_columns,
        feature_columns,
        preprocessing_config
    )
    
    print(f"\nğŸ”§ Preprocessing Results:")
    print(f"  Original size: {preprocessing_info['original_size']}")
    print(f"  Final size: {preprocessing_info['final_size']}")
    print(f"  Reduction rate: {preprocessing_info['reduction_rate']:.2%}")
    print(f"  Steps: {preprocessing_info['steps']}")
    
    # å‰å‡¦ç†è¦ç´„ã‚’å–å¾—
    summary = preprocessor.get_preprocessing_summary(preprocessing_info)
    print(f"\nğŸ“ˆ Preprocessing Summary:")
    for key, value in summary.items():
        print(f"  {key}: {value}")
    
    return processed_data, preprocessing_info


def demonstrate_integrated_pipeline():
    """çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\n" + "="*60)
    print("ğŸš€ Integrated Data Processing Pipeline")
    print("="*60)
    
    # 1. ãƒ‡ãƒ¼ã‚¿å–å¾—
    print("\n1ï¸âƒ£ Data Acquisition:")
    synthetic_data = create_synthetic_data(500)
    print(f"   Retrieved {len(synthetic_data)} compounds")
    
    # 2. åˆ†å­ç‰¹å¾´é‡æŠ½å‡º
    print("\n2ï¸âƒ£ Molecular Feature Extraction:")
    features = MolecularFeatures()
    
    # ã‚µãƒ³ãƒ—ãƒ«åˆ†å­ã®ç‰¹å¾´é‡ã‚’æŠ½å‡º
    sample_smiles = synthetic_data['canonical_smiles'].head(10).tolist()
    feature_df = features.process_molecule_batch(
        sample_smiles,
        include_3d=False,
        include_fingerprints=True,
        include_scaffolds=True
    )
    
    print(f"   Extracted features for {len(feature_df)} molecules")
    print(f"   Feature columns: {len(feature_df.columns)}")
    
    # 3. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    print("\n3ï¸âƒ£ Data Preprocessing:")
    preprocessor = DataPreprocessor()
    
    # ç‰¹å¾´é‡åˆ—ã‚’å®šç¾©
    feature_columns = [col for col in feature_df.columns if col != 'smiles']
    target_columns = ['5HT2A_pIC50', 'D1_pIC50', 'CB1_pIC50', 'MOR_pIC50']
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
    integrated_data = synthetic_data.merge(
        feature_df, 
        left_on='canonical_smiles', 
        right_on='smiles', 
        how='inner'
    )
    
    # å‰å‡¦ç†è¨­å®š
    preprocessing_config = {
        "clean_data": True,
        "handle_outliers": True,
        "impute_missing": True,
        "normalize_features": True,
        "min_pic50": 6.0,
        "max_pic50": 10.0,
        "outlier_method": "iqr",
        "impute_strategy": "median",
        "normalization_method": "standard"
    }
    
    # å‰å‡¦ç†ã‚’å®Ÿè¡Œ
    processed_data, preprocessing_info = preprocessor.preprocess_dataset(
        integrated_data,
        target_columns,
        feature_columns,
        preprocessing_config
    )
    
    print(f"   Processed {len(processed_data)} molecules")
    print(f"   Final features: {len(feature_columns)}")
    
    # 4. çµæœå¯è¦–åŒ–
    print("\n4ï¸âƒ£ Results Visualization:")
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    plt.figure(figsize=(15, 10))
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ¥åˆ†å¸ƒ
    for i, target in enumerate(target_columns, 1):
        plt.subplot(2, 2, i)
        plt.hist(processed_data[target], bins=20, alpha=0.7, edgecolor='black')
        plt.title(f'{target} Distribution')
        plt.xlabel('pIC50')
        plt.ylabel('Frequency')
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('target_distributions.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ç‰¹å¾´é‡ç›¸é–¢ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    plt.figure(figsize=(12, 8))
    correlation_matrix = processed_data[target_columns].corr()
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
    plt.title('Target Correlation Matrix')
    plt.tight_layout()
    plt.savefig('target_correlations.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"\nâœ… Integrated pipeline completed successfully!")
    print(f"   Final dataset: {len(processed_data)} compounds")
    print(f"   Features: {len(feature_columns)}")
    print(f"   Targets: {len(target_columns)}")
    
    return processed_data


def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    print("ğŸ§¬ ChemForge Data Processing Demo")
    print("="*60)
    
    try:
        # 1. ChEMBLãƒ­ãƒ¼ãƒ€ãƒ¼ãƒ‡ãƒ¢
        synthetic_data = demonstrate_chembl_loader()
        
        # 2. åˆ†å­ç‰¹å¾´é‡æŠ½å‡ºãƒ‡ãƒ¢
        features = demonstrate_molecular_features()
        
        # 3. RDKitè¨˜è¿°å­ãƒ‡ãƒ¢
        descriptors = demonstrate_rdkit_descriptors()
        
        # 4. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‡ãƒ¢
        processed_data, preprocessing_info = demonstrate_data_preprocessing()
        
        # 5. çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¢
        final_data = demonstrate_integrated_pipeline()
        
        print("\nğŸ‰ All demonstrations completed successfully!")
        print("ChemForge data processing modules are ready for use!")
        
    except Exception as e:
        logger.error(f"Error in demo: {str(e)}")
        raise


if __name__ == "__main__":
    main()
