"""
PWA+PET Transformer ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
MNISTç ”ç©¶ã§é–‹ç™ºã•ã‚ŒãŸPWA+PETæŠ€è¡“ã‚’CNSå‰µè–¬ã«é©ç”¨ã—ãŸä¾‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple
import time

from chemforge.core.transformer_model import TransformerRegressor
from chemforge.core.pwa_pet_attention import PWA_PET_Attention
from chemforge.core.su2_gate import SU2Gate


def create_synthetic_molecular_data(
    num_samples: int = 1000,
    input_dim: int = 2279,
    num_targets: int = 13
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    åˆæˆåˆ†å­ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    
    Args:
        num_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°
        input_dim: å…¥åŠ›æ¬¡å…ƒï¼ˆåˆ†å­ç‰¹å¾´é‡ï¼‰
        num_targets: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ•°ï¼ˆCNSå—å®¹ä½“ï¼‰
    
    Returns:
        (features, targets): ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
    """
    print(f"ğŸ§¬ åˆæˆåˆ†å­ãƒ‡ãƒ¼ã‚¿ä½œæˆä¸­... ({num_samples} samples)")
    
    # åˆ†å­ç‰¹å¾´é‡ï¼ˆRDKitè¨˜è¿°å­é¢¨ï¼‰
    features = torch.randn(num_samples, input_dim)
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ï¼ˆpIC50é¢¨ã€6-10ã®ç¯„å›²ï¼‰
    targets = torch.randn(num_samples, num_targets) * 1.0 + 8.0
    targets = torch.clamp(targets, 6.0, 10.0)
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†: {features.shape} -> {targets.shape}")
    return features, targets


def compare_architectures(
    features: torch.Tensor,
    targets: torch.Tensor,
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
) -> Dict[str, Dict]:
    """
    ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¯”è¼ƒï¼ˆVanilla Transformer vs PWA+PETï¼‰
    
    Args:
        features: å…¥åŠ›ç‰¹å¾´é‡
        targets: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤
        device: ãƒ‡ãƒã‚¤ã‚¹
    
    Returns:
        æ¯”è¼ƒçµæœ
    """
    print("\nğŸ”¬ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¯”è¼ƒé–‹å§‹...")
    
    results = {}
    batch_size = 32
    num_epochs = 5
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    dataset = torch.utils.data.TensorDataset(features, targets)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # 1. Vanilla Transformer
    print("\nğŸ“Š Vanilla Transformer ãƒ†ã‚¹ãƒˆ...")
    model_vanilla = TransformerRegressor(
        input_dim=2279,
        hidden_dim=256,
        num_layers=4,
        num_heads=8,
        num_targets=13,
        use_pwa_pet=False
    ).to(device)
    
    vanilla_results = train_and_evaluate(
        model_vanilla, dataloader, num_epochs, device, "Vanilla Transformer"
    )
    results["vanilla"] = vanilla_results
    
    # 2. PWA+PET Transformer
    print("\nğŸš€ PWA+PET Transformer ãƒ†ã‚¹ãƒˆ...")
    model_pwa_pet = TransformerRegressor(
        input_dim=2279,
        hidden_dim=256,
        num_layers=4,
        num_heads=8,
        num_targets=13,
        use_pwa_pet=True,
        buckets={"trivial": 2, "fund": 4, "adj": 2},
        use_rope=True,
        use_pet=True,
        pet_curv_reg=1e-5
    ).to(device)
    
    pwa_pet_results = train_and_evaluate(
        model_pwa_pet, dataloader, num_epochs, device, "PWA+PET Transformer"
    )
    results["pwa_pet"] = pwa_pet_results
    
    return results


def train_and_evaluate(
    model: nn.Module,
    dataloader: torch.utils.data.DataLoader,
    num_epochs: int,
    device: str,
    model_name: str
) -> Dict:
    """
    ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡
    
    Args:
        model: ãƒ¢ãƒ‡ãƒ«
        dataloader: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        num_epochs: ã‚¨ãƒãƒƒã‚¯æ•°
        device: ãƒ‡ãƒã‚¤ã‚¹
        model_name: ãƒ¢ãƒ‡ãƒ«å
    
    Returns:
        è©•ä¾¡çµæœ
    """
    criterion = nn.MSELoss()
    optimizer = optim.AdamW(model.parameters(), lr=1e-4)
    
    train_losses = []
    train_times = []
    
    model.train()
    
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        epoch_start = time.time()
        
        for batch_idx, (features, targets) in enumerate(dataloader):
            features, targets = features.to(device), targets.to(device)
            
            optimizer.zero_grad()
            
            # å‰å‘ããƒ‘ã‚¹
            if hasattr(model, 'use_pwa_pet') and model.use_pwa_pet:
                outputs, reg_loss = model(features)
                loss = criterion(outputs, targets) + reg_loss
            else:
                outputs = model(features)
                loss = criterion(outputs, targets)
            
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        epoch_time = time.time() - epoch_start
        avg_loss = epoch_loss / len(dataloader)
        
        train_losses.append(avg_loss)
        train_times.append(epoch_time)
        
        print(f"  Epoch {epoch+1}/{num_epochs}: Loss={avg_loss:.4f}, Time={epoch_time:.2f}s")
    
    # è©•ä¾¡
    model.eval()
    with torch.no_grad():
        total_loss = 0.0
        for features, targets in dataloader:
            features, targets = features.to(device), targets.to(device)
            
            if hasattr(model, 'use_pwa_pet') and model.use_pwa_pet:
                outputs, _ = model(features)
            else:
                outputs = model(features)
            
            loss = criterion(outputs, targets)
            total_loss += loss.item()
        
        avg_test_loss = total_loss / len(dataloader)
    
    return {
        "model_name": model_name,
        "final_loss": avg_test_loss,
        "train_losses": train_losses,
        "train_times": train_times,
        "avg_epoch_time": np.mean(train_times),
        "total_time": np.sum(train_times)
    }


def demonstrate_pwa_pet_components():
    """
    PWA+PETã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å€‹åˆ¥ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    print("\nğŸ”§ PWA+PET ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    
    # 1. SU2Gate ãƒ‡ãƒ¢
    print("\n1ï¸âƒ£ SU2Gate ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    su2_gate = SU2Gate(d_model=64, gate_type='shared')
    
    x = torch.randn(2, 10, 64)
    y = su2_gate(x)
    
    print(f"  å…¥åŠ›å½¢çŠ¶: {x.shape}")
    print(f"  å‡ºåŠ›å½¢çŠ¶: {y.shape}")
    print(f"  ã‚¹ãƒšã‚¯ãƒˆãƒ«åŠå¾„: {su2_gate.get_spectral_radius():.6f}")
    
    # ãƒãƒ«ãƒ ä¿å­˜ãƒã‚§ãƒƒã‚¯
    norm_before = torch.norm(x, dim=-1)
    norm_after = torch.norm(y, dim=-1)
    norm_diff = torch.abs(norm_before - norm_after).max()
    print(f"  ãƒãƒ«ãƒ ä¿å­˜èª¤å·®: {norm_diff:.6f}")
    
    # 2. PWA Attention ãƒ‡ãƒ¢
    print("\n2ï¸âƒ£ PWA Attention ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    attention = PWA_PET_Attention(
        d_model=128,
        n_heads=8,
        buckets={"trivial": 2, "fund": 4, "adj": 2},
        use_rope=True,
        use_pet=True
    )
    
    x_attn = torch.randn(2, 20, 128)
    output, reg_loss = attention(x_attn)
    
    print(f"  å…¥åŠ›å½¢çŠ¶: {x_attn.shape}")
    print(f"  å‡ºåŠ›å½¢çŠ¶: {output.shape}")
    print(f"  æ­£å‰‡åŒ–æå¤±: {reg_loss.item():.6f}")
    print(f"  ãƒã‚±ãƒƒãƒˆè¨­å®š: {attention.router.groups()}")
    
    # 3. RoPE ãƒ‡ãƒ¢
    print("\n3ï¸âƒ£ RoPE ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    from chemforge.core.pwa_pet_attention import RoPE
    
    rope = RoPE(head_dim=64)
    q = torch.randn(2, 4, 20, 64)
    k = torch.randn(2, 4, 20, 64)
    
    q_rotated, k_rotated = rope(q, k, 20)
    
    print(f"  Qå½¢çŠ¶: {q.shape}")
    print(f"  Kå½¢çŠ¶: {k.shape}")
    print(f"  å›è»¢å¾ŒQå½¢çŠ¶: {q_rotated.shape}")
    print(f"  å›è»¢å¾ŒKå½¢çŠ¶: {k_rotated.shape}")


def plot_comparison_results(results: Dict[str, Dict]):
    """
    æ¯”è¼ƒçµæœã®å¯è¦–åŒ–
    
    Args:
        results: æ¯”è¼ƒçµæœ
    """
    print("\nğŸ“ˆ çµæœå¯è¦–åŒ–ä¸­...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('PWA+PET Transformer vs Vanilla Transformer æ¯”è¼ƒ', fontsize=16)
    
    # 1. æå¤±æ›²ç·š
    ax1 = axes[0, 0]
    for model_name, result in results.items():
        ax1.plot(result["train_losses"], label=result["model_name"], marker='o')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Training Loss')
    ax1.set_title('Training Loss Comparison')
    ax1.legend()
    ax1.grid(True)
    
    # 2. ã‚¨ãƒãƒƒã‚¯æ™‚é–“æ¯”è¼ƒ
    ax2 = axes[0, 1]
    model_names = [result["model_name"] for result in results.values()]
    epoch_times = [result["avg_epoch_time"] for result in results.values()]
    colors = ['skyblue', 'lightcoral']
    
    bars = ax2.bar(model_names, epoch_times, color=colors)
    ax2.set_ylabel('Average Epoch Time (s)')
    ax2.set_title('Speed Comparison')
    
    # ãƒãƒ¼ã®ä¸Šã«å€¤ã‚’è¡¨ç¤º
    for bar, time in zip(bars, epoch_times):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{time:.2f}s', ha='center', va='bottom')
    
    # 3. æœ€çµ‚æå¤±æ¯”è¼ƒ
    ax3 = axes[1, 0]
    final_losses = [result["final_loss"] for result in results.values()]
    bars = ax3.bar(model_names, final_losses, color=colors)
    ax3.set_ylabel('Final Test Loss')
    ax3.set_title('Final Performance Comparison')
    
    for bar, loss in zip(bars, final_losses):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                f'{loss:.4f}', ha='center', va='bottom')
    
    # 4. ç·æ™‚é–“æ¯”è¼ƒ
    ax4 = axes[1, 1]
    total_times = [result["total_time"] for result in results.values()]
    bars = ax4.bar(model_names, total_times, color=colors)
    ax4.set_ylabel('Total Training Time (s)')
    ax4.set_title('Total Time Comparison')
    
    for bar, time in zip(bars, total_times):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                f'{time:.1f}s', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('pwa_pet_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("âœ… çµæœã‚’ 'pwa_pet_comparison.png' ã«ä¿å­˜ã—ã¾ã—ãŸ")


def print_performance_summary(results: Dict[str, Dict]):
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ç´„ã®è¡¨ç¤º
    
    Args:
        results: æ¯”è¼ƒçµæœ
    """
    print("\n" + "="*60)
    print("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ç´„")
    print("="*60)
    
    for model_name, result in results.items():
        print(f"\nğŸ”¹ {result['model_name']}")
        print(f"  æœ€çµ‚æå¤±: {result['final_loss']:.6f}")
        print(f"  å¹³å‡ã‚¨ãƒãƒƒã‚¯æ™‚é–“: {result['avg_epoch_time']:.2f}s")
        print(f"  ç·è¨“ç·´æ™‚é–“: {result['total_time']:.2f}s")
    
    # é€Ÿåº¦å‘ä¸Šç‡è¨ˆç®—
    vanilla_time = results["vanilla"]["avg_epoch_time"]
    pwa_pet_time = results["pwa_pet"]["avg_epoch_time"]
    speedup = vanilla_time / pwa_pet_time
    
    print(f"\nğŸš€ PWA+PET Transformer ã®é€Ÿåº¦å‘ä¸Š:")
    print(f"   {speedup:.2f}x é«˜é€ŸåŒ–")
    
    # ç²¾åº¦æ¯”è¼ƒ
    vanilla_loss = results["vanilla"]["final_loss"]
    pwa_pet_loss = results["pwa_pet"]["final_loss"]
    improvement = (vanilla_loss - pwa_pet_loss) / vanilla_loss * 100
    
    print(f"\nğŸ¯ ç²¾åº¦å‘ä¸Š:")
    print(f"   {improvement:.1f}% æ”¹å–„")


def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    print("ğŸ§¬ ChemForge PWA+PET Transformer ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("="*60)
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸  ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # 1. åˆæˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
    features, targets = create_synthetic_molecular_data(
        num_samples=1000,
        input_dim=2279,
        num_targets=13
    )
    
    # 2. ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    demonstrate_pwa_pet_components()
    
    # 3. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¯”è¼ƒ
    results = compare_architectures(features, targets, device)
    
    # 4. çµæœå¯è¦–åŒ–
    plot_comparison_results(results)
    
    # 5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ç´„
    print_performance_summary(results)
    
    print("\nğŸ‰ ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ï¼")
    print("PWA+PET TransformeræŠ€è¡“ãŒCNSå‰µè–¬ã«æˆåŠŸè£ã«é©ç”¨ã•ã‚Œã¾ã—ãŸï¼")


if __name__ == "__main__":
    main()
