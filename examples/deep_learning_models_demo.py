"""
Deep Learning Models Demo

ChemForgeæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
Transformer, GNN, Ensembleãƒ¢ãƒ‡ãƒ«ã®çµ±åˆä¾‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import time
import logging

from chemforge.models.transformer_model import TransformerRegressor
from chemforge.models.gnn_model import GNNRegressor, MolecularGNN, MultiScaleGNN
from chemforge.models.ensemble_model import EnsembleRegressor, HybridEnsemble, AdaptiveEnsemble
from chemforge.models.model_factory import ModelFactory

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_synthetic_data(num_samples: int = 1000) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
    
    Args:
        num_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°
    
    Returns:
        (ç‰¹å¾´é‡, ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ)
    """
    logger.info(f"Creating synthetic data with {num_samples} samples")
    
    # ç‰¹å¾´é‡ï¼ˆ2279æ¬¡å…ƒï¼‰
    features = torch.randn(num_samples, 2279)
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆ13ç¨®é¡ã®CNSã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰
    targets = torch.randn(num_samples, 13) * 1.0 + 8.0
    targets = torch.clamp(targets, 6.0, 10.0)
    
    logger.info(f"Data created: {features.shape} -> {targets.shape}")
    return features, targets


def create_synthetic_graph_data(num_samples: int = 1000) -> List[Dict]:
    """
    åˆæˆã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
    
    Args:
        num_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°
    
    Returns:
        ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
    """
    logger.info(f"Creating synthetic graph data with {num_samples} samples")
    
    graph_data = []
    
    for i in range(num_samples):
        # ãƒ©ãƒ³ãƒ€ãƒ ãªã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚º
        num_nodes = np.random.randint(10, 50)
        
        # ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡
        node_features = torch.randn(num_nodes, 100)
        
        # ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        num_edges = np.random.randint(num_nodes, num_nodes * 3)
        edge_index = torch.randint(0, num_nodes, (2, num_edges))
        
        # ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        batch = torch.zeros(num_nodes, dtype=torch.long)
        
        graph_data.append({
            'x': node_features,
            'edge_index': edge_index,
            'batch': batch
        })
    
    logger.info(f"Graph data created: {len(graph_data)} graphs")
    return graph_data


def demonstrate_transformer_models():
    """Transformerãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\n" + "="*60)
    print("ğŸ¤– Transformer Models Demo")
    print("="*60)
    
    # ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    features, targets = create_synthetic_data(500)
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    train_size = int(0.8 * len(features))
    train_features = features[:train_size]
    train_targets = targets[:train_size]
    test_features = features[train_size:]
    test_targets = targets[train_size:]
    
    # 1. æ¨™æº–Transformer
    print("\nğŸ”¹ Standard Transformer:")
    standard_transformer = TransformerRegressor(
        input_dim=2279,
        hidden_dim=256,
        num_layers=4,
        num_heads=8,
        num_targets=13,
        use_pwa_pet=False,
        dropout=0.1
    )
    
    # è¨“ç·´
    criterion = nn.MSELoss()
    optimizer = optim.Adam(standard_transformer.parameters(), lr=1e-4)
    
    start_time = time.time()
    for epoch in range(10):
        optimizer.zero_grad()
        outputs = standard_transformer(train_features)
        loss = criterion(outputs, train_targets)
        loss.backward()
        optimizer.step()
        
        if epoch % 5 == 0:
            print(f"  Epoch {epoch}: Loss = {loss.item():.4f}")
    
    training_time = time.time() - start_time
    
    # è©•ä¾¡
    with torch.no_grad():
        test_outputs = standard_transformer(test_features)
        test_loss = criterion(test_outputs, test_targets)
    
    print(f"  Training time: {training_time:.2f}s")
    print(f"  Test loss: {test_loss.item():.4f}")
    
    # 2. PWA+PET Transformer
    print("\nğŸ”¹ PWA+PET Transformer:")
    pwa_pet_transformer = TransformerRegressor(
        input_dim=2279,
        hidden_dim=256,
        num_layers=4,
        num_heads=8,
        num_targets=13,
        use_pwa_pet=True,
        buckets={"trivial": 2, "fund": 4, "adj": 2},
        use_rope=True,
        use_pet=True,
        pet_curv_reg=1e-5,
        dropout=0.1
    )
    
    # è¨“ç·´
    optimizer = optim.Adam(pwa_pet_transformer.parameters(), lr=1e-4)
    
    start_time = time.time()
    for epoch in range(10):
        optimizer.zero_grad()
        outputs, reg_loss = pwa_pet_transformer(train_features)
        loss = criterion(outputs, train_targets) + reg_loss
        loss.backward()
        optimizer.step()
        
        if epoch % 5 == 0:
            print(f"  Epoch {epoch}: Loss = {loss.item():.4f}")
    
    training_time = time.time() - start_time
    
    # è©•ä¾¡
    with torch.no_grad():
        test_outputs, _ = pwa_pet_transformer(test_features)
        test_loss = criterion(test_outputs, test_targets)
    
    print(f"  Training time: {training_time:.2f}s")
    print(f"  Test loss: {test_loss.item():.4f}")
    
    return standard_transformer, pwa_pet_transformer


def demonstrate_gnn_models():
    """GNNãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\n" + "="*60)
    print("ğŸ•¸ï¸ GNN Models Demo")
    print("="*60)
    
    # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    graph_data = create_synthetic_graph_data(200)
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    train_size = int(0.8 * len(graph_data))
    train_data = graph_data[:train_size]
    test_data = graph_data[train_size:]
    
    # 1. æ¨™æº–GNN
    print("\nğŸ”¹ Standard GNN:")
    standard_gnn = GNNRegressor(
        input_dim=100,
        hidden_dim=128,
        num_layers=3,
        num_heads=4,
        num_targets=13,
        gnn_type="gcn",
        dropout=0.1
    )
    
    # è¨“ç·´
    criterion = nn.MSELoss()
    optimizer = optim.Adam(standard_gnn.parameters(), lr=1e-4)
    
    start_time = time.time()
    for epoch in range(10):
        total_loss = 0
        for graph in train_data:
            optimizer.zero_grad()
            outputs = standard_gnn(graph['x'], graph['edge_index'], graph['batch'])
            targets = torch.randn(1, 13)  # ãƒ€ãƒŸãƒ¼ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        if epoch % 5 == 0:
            print(f"  Epoch {epoch}: Loss = {total_loss/len(train_data):.4f}")
    
    training_time = time.time() - start_time
    print(f"  Training time: {training_time:.2f}s")
    
    # 2. GAT
    print("\nğŸ”¹ GAT (Graph Attention Network):")
    gat_gnn = GNNRegressor(
        input_dim=100,
        hidden_dim=128,
        num_layers=3,
        num_heads=4,
        num_targets=13,
        gnn_type="gat",
        dropout=0.1
    )
    
    # è¨“ç·´
    optimizer = optim.Adam(gat_gnn.parameters(), lr=1e-4)
    
    start_time = time.time()
    for epoch in range(10):
        total_loss = 0
        for graph in train_data:
            optimizer.zero_grad()
            outputs = gat_gnn(graph['x'], graph['edge_index'], graph['batch'])
            targets = torch.randn(1, 13)  # ãƒ€ãƒŸãƒ¼ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        if epoch % 5 == 0:
            print(f"  Epoch {epoch}: Loss = {total_loss/len(train_data):.4f}")
    
    training_time = time.time() - start_time
    print(f"  Training time: {training_time:.2f}s")
    
    # 3. ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«GNN
    print("\nğŸ”¹ Multi-Scale GNN:")
    multiscale_gnn = MultiScaleGNN(
        input_dim=100,
        hidden_dim=128,
        num_layers=3,
        num_targets=13,
        scales=[1, 2, 3],
        dropout=0.1
    )
    
    # è¨“ç·´
    optimizer = optim.Adam(multiscale_gnn.parameters(), lr=1e-4)
    
    start_time = time.time()
    for epoch in range(10):
        total_loss = 0
        for graph in train_data:
            optimizer.zero_grad()
            outputs = multiscale_gnn(graph['x'], graph['edge_index'], graph['batch'])
            targets = torch.randn(1, 13)  # ãƒ€ãƒŸãƒ¼ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        if epoch % 5 == 0:
            print(f"  Epoch {epoch}: Loss = {total_loss/len(train_data):.4f}")
    
    training_time = time.time() - start_time
    print(f"  Training time: {training_time:.2f}s")
    
    return standard_gnn, gat_gnn, multiscale_gnn


def demonstrate_ensemble_models():
    """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\n" + "="*60)
    print("ğŸ¯ Ensemble Models Demo")
    print("="*60)
    
    # ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    features, targets = create_synthetic_data(500)
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    train_size = int(0.8 * len(features))
    train_features = features[:train_size]
    train_targets = targets[:train_size]
    test_features = features[train_size:]
    test_targets = targets[train_size:]
    
    # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    transformer = TransformerRegressor(
        input_dim=2279,
        hidden_dim=128,
        num_layers=3,
        num_heads=4,
        num_targets=13,
        use_pwa_pet=False,
        dropout=0.1
    )
    
    # 1. é‡ã¿ä»˜ãå¹³å‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    print("\nğŸ”¹ Weighted Average Ensemble:")
    weighted_ensemble = EnsembleRegressor(
        models=[transformer, transformer],  # åŒã˜ãƒ¢ãƒ‡ãƒ«ã‚’2ã¤ä½¿ç”¨
        ensemble_method="weighted_average",
        weights=[0.6, 0.4]
    )
    
    # è¨“ç·´
    criterion = nn.MSELoss()
    optimizer = optim.Adam(weighted_ensemble.parameters(), lr=1e-4)
    
    start_time = time.time()
    for epoch in range(10):
        optimizer.zero_grad()
        outputs = weighted_ensemble(train_features)
        loss = criterion(outputs, train_targets)
        loss.backward()
        optimizer.step()
        
        if epoch % 5 == 0:
            print(f"  Epoch {epoch}: Loss = {loss.item():.4f}")
    
    training_time = time.time() - start_time
    
    # è©•ä¾¡
    with torch.no_grad():
        test_outputs = weighted_ensemble(test_features)
        test_loss = criterion(test_outputs, test_targets)
    
    print(f"  Training time: {training_time:.2f}s")
    print(f"  Test loss: {test_loss.item():.4f}")
    
    # 2. ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    print("\nğŸ”¹ Stacking Ensemble:")
    stacking_ensemble = EnsembleRegressor(
        models=[transformer, transformer],
        ensemble_method="stacking",
        use_meta_learning=True,
        meta_hidden_dim=128
    )
    
    # è¨“ç·´
    optimizer = optim.Adam(stacking_ensemble.parameters(), lr=1e-4)
    
    start_time = time.time()
    for epoch in range(10):
        optimizer.zero_grad()
        outputs = stacking_ensemble(train_features)
        loss = criterion(outputs, train_targets)
        loss.backward()
        optimizer.step()
        
        if epoch % 5 == 0:
            print(f"  Epoch {epoch}: Loss = {loss.item():.4f}")
    
    training_time = time.time() - start_time
    
    # è©•ä¾¡
    with torch.no_grad():
        test_outputs = stacking_ensemble(test_features)
        test_loss = criterion(test_outputs, test_targets)
    
    print(f"  Training time: {training_time:.2f}s")
    print(f"  Test loss: {test_loss.item():.4f}")
    
    # 3. é©å¿œçš„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    print("\nğŸ”¹ Adaptive Ensemble:")
    adaptive_ensemble = AdaptiveEnsemble(
        models=[transformer, transformer],
        input_dim=2279,
        hidden_dim=128,
        num_targets=13
    )
    
    # è¨“ç·´
    optimizer = optim.Adam(adaptive_ensemble.parameters(), lr=1e-4)
    
    start_time = time.time()
    for epoch in range(10):
        optimizer.zero_grad()
        outputs = adaptive_ensemble(train_features)
        loss = criterion(outputs, train_targets)
        loss.backward()
        optimizer.step()
        
        if epoch % 5 == 0:
            print(f"  Epoch {epoch}: Loss = {loss.item():.4f}")
    
    training_time = time.time() - start_time
    
    # è©•ä¾¡
    with torch.no_grad():
        test_outputs = adaptive_ensemble(test_features)
        test_loss = criterion(test_outputs, test_targets)
    
    print(f"  Training time: {training_time:.2f}s")
    print(f"  Test loss: {test_loss.item():.4f}")
    
    # é©å¿œçš„é‡ã¿ã‚’è¡¨ç¤º
    with torch.no_grad():
        weights = adaptive_ensemble.get_adaptive_weights(test_features[:5])
        print(f"  Adaptive weights (first 5 samples):")
        for i, weight in enumerate(weights):
            print(f"    Sample {i}: {weight.numpy()}")
    
    return weighted_ensemble, stacking_ensemble, adaptive_ensemble


def demonstrate_model_factory():
    """ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\n" + "="*60)
    print("ğŸ­ Model Factory Demo")
    print("="*60)
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã‚’åˆæœŸåŒ–
    factory = ModelFactory()
    
    # 1. è¨­å®šã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    print("\nğŸ”¹ Creating models from config:")
    
    # Transformerãƒ¢ãƒ‡ãƒ«
    transformer = factory.create_transformer(
        input_dim=2279,
        hidden_dim=128,
        num_layers=3,
        num_heads=4,
        num_targets=13,
        use_pwa_pet=True
    )
    
    print(f"  Transformer created: {type(transformer).__name__}")
    
    # GNNãƒ¢ãƒ‡ãƒ«
    gnn = factory.create_gnn(
        input_dim=100,
        hidden_dim=128,
        num_layers=3,
        num_heads=4,
        num_targets=13,
        gnn_type="gat"
    )
    
    print(f"  GNN created: {type(gnn).__name__}")
    
    # 2. ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—
    print("\nğŸ”¹ Model information:")
    
    transformer_info = factory.get_model_info(transformer)
    print(f"  Transformer:")
    print(f"    Parameters: {transformer_info['num_parameters']:,}")
    print(f"    Trainable: {transformer_info['trainable_parameters']:,}")
    print(f"    Size: {transformer_info['model_size_mb']:.2f} MB")
    
    gnn_info = factory.get_model_info(gnn)
    print(f"  GNN:")
    print(f"    Parameters: {gnn_info['num_parameters']:,}")
    print(f"    Trainable: {gnn_info['trainable_parameters']:,}")
    print(f"    Size: {gnn_info['model_size_mb']:.2f} MB")
    
    # 3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    print("\nğŸ”¹ Creating ensemble model:")
    
    ensemble = factory.create_ensemble(
        models=[transformer, transformer],
        ensemble_method="weighted_average",
        weights=[0.6, 0.4]
    )
    
    print(f"  Ensemble created: {type(ensemble).__name__}")
    
    # 4. ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ä¿å­˜
    print("\nğŸ”¹ Saving model config:")
    
    factory.save_model_config(transformer, "transformer_config.yaml")
    factory.save_model_config(gnn, "gnn_config.yaml")
    factory.save_model_config(ensemble, "ensemble_config.yaml")
    
    print("  Model configs saved successfully")
    
    return factory, transformer, gnn, ensemble


def plot_model_comparison(models: Dict[str, torch.nn.Module], test_features: torch.Tensor, test_targets: torch.Tensor):
    """
    ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    
    Args:
        models: ãƒ¢ãƒ‡ãƒ«è¾æ›¸
        test_features: ãƒ†ã‚¹ãƒˆç‰¹å¾´é‡
        test_targets: ãƒ†ã‚¹ãƒˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
    """
    print("\n" + "="*60)
    print("ğŸ“Š Model Comparison Visualization")
    print("="*60)
    
    # äºˆæ¸¬ã‚’å–å¾—
    predictions = {}
    for name, model in models.items():
        with torch.no_grad():
            if hasattr(model, 'forward'):
                pred = model(test_features)
                if isinstance(pred, tuple):
                    pred = pred[0]
                predictions[name] = pred
            else:
                predictions[name] = test_targets  # ãƒ€ãƒŸãƒ¼
    
    # äºˆæ¸¬ vs å®Ÿéš›ã®å€¤ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.flatten()
    
    model_names = list(predictions.keys())
    for i, (name, pred) in enumerate(predictions.items()):
        if i >= 4:
            break
        
        ax = axes[i]
        
        # æœ€åˆã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ã¿ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        true_values = test_targets[:, 0].numpy()
        pred_values = pred[:, 0].numpy()
        
        ax.scatter(true_values, pred_values, alpha=0.6, s=20)
        ax.plot([6, 10], [6, 10], 'r--', alpha=0.8)
        ax.set_xlabel(f"True Target 0")
        ax.set_ylabel(f"Predicted Target 0")
        ax.set_title(f"{name}")
        ax.grid(True, alpha=0.3)
        
        # RÂ²ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        r2 = 1 - np.sum((pred_values - true_values) ** 2) / np.sum((true_values - np.mean(true_values)) ** 2)
        ax.text(0.05, 0.95, f"RÂ² = {r2:.3f}", transform=ax.transAxes, 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
    
    # ä½™åˆ†ãªã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º
    for i in range(len(model_names), 4):
        axes[i].set_visible(False)
    
    plt.suptitle("Model Comparison - Prediction vs True Values", fontsize=16)
    plt.tight_layout()
    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()


def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    print("ğŸ§¬ ChemForge Deep Learning Models Demo")
    print("="*60)
    
    try:
        # 1. Transformerãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¢
        standard_transformer, pwa_pet_transformer = demonstrate_transformer_models()
        
        # 2. GNNãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¢
        standard_gnn, gat_gnn, multiscale_gnn = demonstrate_gnn_models()
        
        # 3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¢
        weighted_ensemble, stacking_ensemble, adaptive_ensemble = demonstrate_ensemble_models()
        
        # 4. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ‡ãƒ¢
        factory, transformer, gnn, ensemble = demonstrate_model_factory()
        
        # 5. ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå¯è¦–åŒ–
        test_features, test_targets = create_synthetic_data(100)
        models = {
            "Standard Transformer": standard_transformer,
            "PWA+PET Transformer": pwa_pet_transformer,
            "Weighted Ensemble": weighted_ensemble,
            "Adaptive Ensemble": adaptive_ensemble
        }
        
        plot_model_comparison(models, test_features, test_targets)
        
        print("\nğŸ‰ All demonstrations completed successfully!")
        print("ChemForge deep learning models are ready for use!")
        
    except Exception as e:
        logger.error(f"Error in demo: {str(e)}")
        raise


if __name__ == "__main__":
    main()
