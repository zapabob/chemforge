"""
Training Demo

ChemForgeå­¦ç¿’ãƒ»æ¨è«–ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
åŒ…æ‹¬çš„ãªå­¦ç¿’ãƒ»æ¨è«–ãƒ»è©•ä¾¡ã®å®Ÿè£…ä¾‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Union, Any
import time
import logging
from pathlib import Path
import tempfile
import shutil

from chemforge.training.trainer import Trainer
from chemforge.training.loss_functions import LossFunctions
from chemforge.training.metrics import Metrics
from chemforge.training.optimizer import OptimizerManager
from chemforge.training.scheduler import SchedulerManager
from chemforge.training.checkpoint import CheckpointManager

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DemoModel(nn.Module):
    """ãƒ‡ãƒ¢ç”¨ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, input_size: int = 10, hidden_size: int = 64, output_size: int = 1):
        super(DemoModel, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, hidden_size)
        self.linear3 = nn.Linear(hidden_size, output_size)
        
        # æ´»æ€§åŒ–é–¢æ•°
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
        
        # åˆæœŸåŒ–
        self._initialize_weights()
    
    def _initialize_weights(self):
        """é‡ã¿ã‚’åˆæœŸåŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        x = self.linear1(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        x = self.linear2(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        x = self.linear3(x)
        return x


def create_demo_data(
    num_samples: int = 1000,
    input_size: int = 10,
    output_size: int = 1,
    noise_level: float = 0.1
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    ãƒ‡ãƒ¢ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    
    Args:
        num_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°
        input_size: å…¥åŠ›ã‚µã‚¤ã‚º
        output_size: å‡ºåŠ›ã‚µã‚¤ã‚º
        noise_level: ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«
    
    Returns:
        å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿
    """
    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    X = torch.randn(num_samples, input_size)
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆç·šå½¢é–¢ä¿‚ + ãƒã‚¤ã‚ºï¼‰
    true_weights = torch.randn(input_size, output_size)
    y = torch.mm(X, true_weights) + noise_level * torch.randn(num_samples, output_size)
    
    return X, y


def create_data_loaders(
    X: torch.Tensor,
    y: torch.Tensor,
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
    batch_size: int = 32
) -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, torch.utils.data.DataLoader]:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ
    
    Args:
        X: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿
        train_ratio: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ¯”ç‡
        val_ratio: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æ¯”ç‡
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
    
    Returns:
        å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    """
    # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
    num_samples = len(X)
    train_size = int(num_samples * train_ratio)
    val_size = int(num_samples * val_ratio)
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
    indices = torch.randperm(num_samples)
    
    train_indices = indices[:train_size]
    val_indices = indices[train_size:train_size + val_size]
    test_indices = indices[train_size + val_size:]
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    train_dataset = torch.utils.data.TensorDataset(X[train_indices], y[train_indices])
    val_dataset = torch.utils.data.TensorDataset(X[val_indices], y[val_indices])
    test_dataset = torch.utils.data.TensorDataset(X[test_indices], y[test_indices])
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True
    )
    val_loader = torch.utils.data.DataLoader(
        val_dataset, batch_size=batch_size, shuffle=False
    )
    test_loader = torch.utils.data.DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False
    )
    
    return train_loader, val_loader, test_loader


def demonstrate_training_system():
    """å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\n" + "="*60)
    print("ğŸš€ Training System Demo")
    print("="*60)
    
    # ãƒ‡ãƒ¢ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    print("\nğŸ”¹ Creating demo data...")
    X, y = create_demo_data(num_samples=1000, input_size=10, output_size=1)
    print(f"  Data shape: X={X.shape}, y={y.shape}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ
    print("\nğŸ”¹ Creating data loaders...")
    train_loader, val_loader, test_loader = create_data_loaders(X, y)
    print(f"  Train batches: {len(train_loader)}")
    print(f"  Val batches: {len(val_loader)}")
    print(f"  Test batches: {len(test_loader)}")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    print("\nğŸ”¹ Creating model...")
    model = DemoModel(input_size=10, hidden_size=64, output_size=1)
    print(f"  Model parameters: {sum(p.numel() for p in model.parameters())}")
    print(f"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}")
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’åˆæœŸåŒ–
    print("\nğŸ”¹ Initializing trainer...")
    trainer = Trainer(
        model=model,
        device="cpu",
        use_amp=False,
        checkpoint_dir="demo_checkpoints",
        log_dir="demo_logs",
        save_best=True,
        patience=10,
        min_delta=1e-4
    )
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’è¨­å®š
    print("\nğŸ”¹ Setting up optimizer...")
    trainer.setup_optimizer(
        optimizer_type="adamw",
        learning_rate=1e-3,
        weight_decay=1e-4
    )
    
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’è¨­å®š
    print("\nğŸ”¹ Setting up scheduler...")
    trainer.setup_scheduler(
        scheduler_type="cosine",
        T_max=50
    )
    
    # æå¤±é–¢æ•°ã‚’è¨­å®š
    print("\nğŸ”¹ Setting up loss function...")
    trainer.setup_loss_function("mse")
    
    # å­¦ç¿’ã‚’å®Ÿè¡Œ
    print("\nğŸ”¹ Starting training...")
    start_time = time.time()
    
    history = trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=50,
        save_frequency=10
    )
    
    training_time = time.time() - start_time
    print(f"  Training completed in {training_time:.2f}s")
    
    # å­¦ç¿’å±¥æ­´ã‚’è¡¨ç¤º
    print("\nğŸ”¹ Training history:")
    print(f"  Final train loss: {history['train_loss'][-1]:.4f}")
    print(f"  Final val loss: {history['val_loss'][-1]:.4f}")
    print(f"  Best val loss: {min(history['val_loss']):.4f}")
    
    # è©•ä¾¡ã‚’å®Ÿè¡Œ
    print("\nğŸ”¹ Evaluating model...")
    test_metrics = trainer.evaluate(test_loader)
    print(f"  Test loss: {test_metrics['loss']:.4f}")
    print(f"  Test RÂ²: {test_metrics['r2']:.4f}")
    print(f"  Test MAE: {test_metrics['mae']:.4f}")
    
    # äºˆæ¸¬ã‚’å®Ÿè¡Œ
    print("\nğŸ”¹ Making predictions...")
    predictions = trainer.predict(test_loader)
    print(f"  Predictions shape: {predictions.shape}")
    print(f"  Predictions mean: {predictions.mean():.4f}")
    print(f"  Predictions std: {predictions.std():.4f}")
    
    # å­¦ç¿’è¦ç´„ã‚’å–å¾—
    print("\nğŸ”¹ Training summary:")
    summary = trainer.get_training_summary()
    for key, value in summary.items():
        print(f"  {key}: {value}")
    
    return trainer, history, test_metrics


def demonstrate_loss_functions():
    """æå¤±é–¢æ•°ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\n" + "="*60)
    print("ğŸ“Š Loss Functions Demo")
    print("="*60)
    
    # æå¤±é–¢æ•°ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–
    loss_functions = LossFunctions()
    
    print("\nğŸ”¹ Available loss functions:")
    available_losses = loss_functions.get_available_losses()
    for loss_name in available_losses:
        print(f"  - {loss_name}")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    print("\nğŸ”¹ Creating test data...")
    predictions = torch.randn(100, 1)
    targets = torch.randn(100, 1)
    print(f"  Predictions shape: {predictions.shape}")
    print(f"  Targets shape: {targets.shape}")
    
    # å„ç¨®æå¤±é–¢æ•°ã‚’ãƒ†ã‚¹ãƒˆ
    print("\nğŸ”¹ Testing loss functions:")
    test_losses = ['mse', 'mae', 'smooth_l1', 'huber']
    
    for loss_name in test_losses:
        loss_fn = loss_functions.get_loss_function(loss_name)
        loss_value = loss_fn(predictions, targets)
        print(f"  {loss_name}: {loss_value.item():.4f}")
    
    # æå¤±é–¢æ•°æƒ…å ±ã‚’å–å¾—
    print("\nğŸ”¹ Loss function information:")
    for loss_name in test_losses:
        info = loss_functions.get_loss_info(loss_name)
        print(f"  {loss_name}:")
        print(f"    Description: {info['description']}")
        print(f"    Parameters: {info['parameters']}")
        print(f"    Use cases: {info['use_cases']}")


def demonstrate_metrics():
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\n" + "="*60)
    print("ğŸ“ˆ Metrics Demo")
    print("="*60)
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–
    metrics = Metrics()
    
    print("\nğŸ”¹ Available metrics:")
    regression_metrics = metrics.get_available_metrics("regression")
    classification_metrics = metrics.get_available_metrics("classification")
    print(f"  Regression: {regression_metrics}")
    print(f"  Classification: {classification_metrics}")
    
    # å›å¸°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ†ã‚¹ãƒˆ
    print("\nğŸ”¹ Testing regression metrics:")
    predictions = torch.randn(100, 1)
    targets = torch.randn(100, 1)
    
    regression_metrics_result = metrics.calculate_metrics(predictions, targets, "regression")
    for metric_name, metric_value in regression_metrics_result.items():
        print(f"  {metric_name}: {metric_value:.4f}")
    
    # åˆ†é¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ†ã‚¹ãƒˆ
    print("\nğŸ”¹ Testing classification metrics:")
    predictions_cls = torch.randn(100, 5)
    targets_cls = torch.randint(0, 5, (100,))
    
    classification_metrics_result = metrics.calculate_metrics(predictions_cls, targets_cls, "classification")
    for metric_name, metric_value in classification_metrics_result.items():
        print(f"  {metric_name}: {metric_value:.4f}")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã‚’å–å¾—
    print("\nğŸ”¹ Metric information:")
    test_metrics = ['mse', 'r2', 'accuracy', 'f1']
    for metric_name in test_metrics:
        info = metrics.get_metric_info(metric_name)
        print(f"  {metric_name}:")
        print(f"    Description: {info['description']}")
        print(f"    Range: {info['range']}")
        print(f"    Interpretation: {info['interpretation']}")


def demonstrate_optimizer():
    """ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\n" + "="*60)
    print("âš™ï¸ Optimizer Demo")
    print("="*60)
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–
    optimizer_manager = OptimizerManager()
    
    print("\nğŸ”¹ Available optimizers:")
    available_optimizers = optimizer_manager.get_available_optimizers()
    for optimizer_name in available_optimizers:
        print(f"  - {optimizer_name}")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    model = DemoModel(input_size=10, hidden_size=64, output_size=1)
    
    # å„ç¨®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ãƒ†ã‚¹ãƒˆ
    print("\nğŸ”¹ Testing optimizers:")
    test_optimizers = ['adam', 'adamw', 'sgd', 'rmsprop']
    
    for optimizer_name in test_optimizers:
        optimizer = optimizer_manager.create_optimizer(
            model=model,
            optimizer_type=optimizer_name,
            learning_rate=1e-3,
            weight_decay=1e-4
        )
        print(f"  {optimizer_name}: {type(optimizer).__name__}")
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
    print("\nğŸ”¹ Optimizer information:")
    for optimizer_name in test_optimizers:
        info = optimizer_manager.get_optimizer_info(optimizer_name)
        print(f"  {optimizer_name}:")
        print(f"    Description: {info['description']}")
        print(f"    Use cases: {info['use_cases']}")
        print(f"    Advantages: {info['advantages']}")
        print(f"    Disadvantages: {info['disadvantages']}")


def demonstrate_scheduler():
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\n" + "="*60)
    print("ğŸ“… Scheduler Demo")
    print("="*60)
    
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–
    scheduler_manager = SchedulerManager()
    
    print("\nğŸ”¹ Available schedulers:")
    available_schedulers = scheduler_manager.get_available_schedulers()
    for scheduler_name in available_schedulers:
        print(f"  - {scheduler_name}")
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
    model = DemoModel(input_size=10, hidden_size=64, output_size=1)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
    
    # å„ç¨®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’ãƒ†ã‚¹ãƒˆ
    print("\nğŸ”¹ Testing schedulers:")
    test_schedulers = ['cosine', 'step', 'exponential', 'reduce_on_plateau']
    
    for scheduler_name in test_schedulers:
        scheduler = scheduler_manager.create_scheduler(
            optimizer=optimizer,
            scheduler_type=scheduler_name
        )
        print(f"  {scheduler_name}: {type(scheduler).__name__}")
    
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼æƒ…å ±ã‚’å–å¾—
    print("\nğŸ”¹ Scheduler information:")
    for scheduler_name in test_schedulers:
        info = scheduler_manager.get_scheduler_info(scheduler_name)
        print(f"  {scheduler_name}:")
        print(f"    Description: {info['description']}")
        print(f"    Use cases: {info['use_cases']}")
        print(f"    Advantages: {info['advantages']}")
        print(f"    Disadvantages: {info['disadvantages']}")


def demonstrate_checkpoint():
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\n" + "="*60)
    print("ğŸ’¾ Checkpoint Demo")
    print("="*60)
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–
    checkpoint_manager = CheckpointManager(
        checkpoint_dir="demo_checkpoints",
        max_checkpoints=5,
        save_best=True,
        save_frequency=10,
        backup_frequency=100
    )
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
    model = DemoModel(input_size=10, hidden_size=64, output_size=1)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
    
    print("\nğŸ”¹ Testing checkpoint operations:")
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜
    print("  Saving checkpoints...")
    for epoch in range(1, 6):
        score = 1.0 - epoch * 0.1 + np.random.normal(0, 0.05)
        is_best = score < checkpoint_manager.best_score
        
        checkpoint_path = checkpoint_manager.save_checkpoint(
            model=model,
            optimizer=optimizer,
            epoch=epoch,
            score=score,
            is_best=is_best
        )
        print(f"    Epoch {epoch}: score={score:.4f}, is_best={is_best}")
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’å–å¾—
    print("\nğŸ”¹ Checkpoint list:")
    checkpoint_list = checkpoint_manager.get_checkpoint_list()
    for cp in checkpoint_list:
        print(f"  Epoch {cp['epoch']}: score={cp['score']:.4f}, is_best={cp['is_best']}")
    
    # æœ€è‰¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—
    print("\nğŸ”¹ Best checkpoint info:")
    best_info = checkpoint_manager.get_best_checkpoint_info()
    if best_info:
        print(f"  Path: {best_info['path']}")
        print(f"  Score: {best_info['score']:.4f}")
        print(f"  Timestamp: {best_info['timestamp']}")
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè¦ç´„ã‚’å–å¾—
    print("\nğŸ”¹ Checkpoint summary:")
    summary = checkpoint_manager.get_checkpoint_summary()
    for key, value in summary.items():
        print(f"  {key}: {value}")


def plot_training_results(history: Dict[str, List[float]]):
    """
    å­¦ç¿’çµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    
    Args:
        history: å­¦ç¿’å±¥æ­´
    """
    print("\n" + "="*60)
    print("ğŸ“Š Training Results Visualization")
    print("="*60)
    
    # ãƒ—ãƒ­ãƒƒãƒˆè¨­å®š
    plt.style.use('default')
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()
    
    # 1. æå¤±æ›²ç·š
    ax1 = axes[0]
    ax1.plot(history['train_loss'], label='Train Loss', color='blue')
    ax1.plot(history['val_loss'], label='Val Loss', color='red')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training and Validation Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. å­¦ç¿’ç‡ï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆï¼‰
    ax2 = axes[1]
    if 'learning_rate' in history:
        ax2.plot(history['learning_rate'], color='green')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Learning Rate')
        ax2.set_title('Learning Rate Schedule')
        ax2.grid(True, alpha=0.3)
    else:
        ax2.text(0.5, 0.5, 'No learning rate data', ha='center', va='center')
        ax2.set_title('Learning Rate Schedule')
    
    # 3. ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
    ax3 = axes[2]
    if 'train_metrics' in history and history['train_metrics']:
        train_metrics = history['train_metrics']
        if 'r2' in train_metrics[0]:
            r2_scores = [m['r2'] for m in train_metrics]
            ax3.plot(r2_scores, label='Train RÂ²', color='blue')
        if 'val_metrics' in history and history['val_metrics']:
            val_metrics = history['val_metrics']
            if 'r2' in val_metrics[0]:
                val_r2_scores = [m['r2'] for m in val_metrics]
                ax3.plot(val_r2_scores, label='Val RÂ²', color='red')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('RÂ² Score')
        ax3.set_title('RÂ² Score Over Time')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
    else:
        ax3.text(0.5, 0.5, 'No metrics data', ha='center', va='center')
        ax3.set_title('Metrics Over Time')
    
    # 4. æå¤±åˆ†å¸ƒ
    ax4 = axes[3]
    ax4.hist(history['train_loss'], bins=20, alpha=0.7, label='Train Loss', color='blue')
    ax4.hist(history['val_loss'], bins=20, alpha=0.7, label='Val Loss', color='red')
    ax4.set_xlabel('Loss Value')
    ax4.set_ylabel('Frequency')
    ax4.set_title('Loss Distribution')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.suptitle('Training Results Analysis', fontsize=16)
    plt.tight_layout()
    plt.savefig('training_results.png', dpi=300, bbox_inches='tight')
    plt.show()


def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    print("ğŸš€ ChemForge Training System Demo")
    print("="*60)
    
    try:
        # 1. å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ¢
        trainer, history, test_metrics = demonstrate_training_system()
        
        # 2. æå¤±é–¢æ•°ãƒ‡ãƒ¢
        demonstrate_loss_functions()
        
        # 3. ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¢
        demonstrate_metrics()
        
        # 4. ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ãƒ‡ãƒ¢
        demonstrate_optimizer()
        
        # 5. ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãƒ‡ãƒ¢
        demonstrate_scheduler()
        
        # 6. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¢
        demonstrate_checkpoint()
        
        # 7. å­¦ç¿’çµæœå¯è¦–åŒ–
        plot_training_results(history)
        
        print("\nğŸ‰ All training system demonstrations completed successfully!")
        print("ChemForge training system is ready for use!")
        
    except Exception as e:
        logger.error(f"Error in demo: {str(e)}")
        raise
    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            shutil.rmtree("demo_checkpoints", ignore_errors=True)
            shutil.rmtree("demo_logs", ignore_errors=True)
        except:
            pass


if __name__ == "__main__":
    main()

