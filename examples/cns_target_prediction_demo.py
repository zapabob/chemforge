"""
CNSã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
PWA+PET Transformerã‚’ä½¿ç”¨ã—ãŸCNSå—å®¹ä½“ãƒ»ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã®pIC50äºˆæ¸¬
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import time

from chemforge.core.transformer_model import TransformerRegressor
from chemforge.targets.chembl_targets import ChEMBLTargets


class CNSMolecularDataset:
    """
    CNSåˆ†å­ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦CNSã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬ã‚’ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    
    def __init__(self, num_samples: int = 2000):
        self.num_samples = num_samples
        self.cns_targets = [
            '5HT2A', '5HT1A', 'D1', 'D2', 'CB1', 'CB2',
            'MOR', 'DOR', 'KOR', 'NOP', 'SERT', 'DAT', 'NET'
        ]
        self.features, self.targets = self._generate_synthetic_data()
    
    def _generate_synthetic_data(self) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        åˆæˆCNSåˆ†å­ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        
        Returns:
            (features, targets): åˆ†å­ç‰¹å¾´é‡ã¨pIC50å€¤
        """
        print(f"ğŸ§¬ CNSåˆ†å­ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­... ({self.num_samples} samples)")
        
        # åˆ†å­ç‰¹å¾´é‡ï¼ˆ2279æ¬¡å…ƒï¼‰
        features = torch.randn(self.num_samples, 2279)
        
        # CNSã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®pIC50å€¤ï¼ˆ6-10ã®ç¯„å›²ï¼‰
        targets = torch.randn(self.num_samples, len(self.cns_targets)) * 1.0 + 8.0
        targets = torch.clamp(targets, 6.0, 10.0)
        
        # ä¸€éƒ¨ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã§ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ 
        # 5HT2A: ã‚ˆã‚Šé«˜ã„æ´»æ€§ã‚’æŒã¤åŒ–åˆç‰©ã‚’æ¨¡æ“¬
        targets[:, 0] += torch.randn(self.num_samples) * 0.5 + 0.5
        
        # CB1: ã‚ˆã‚Šä½ã„æ´»æ€§ã‚’æŒã¤åŒ–åˆç‰©ã‚’æ¨¡æ“¬
        targets[:, 4] -= torch.randn(self.num_samples) * 0.3 + 0.2
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã‚’6-10ã®ç¯„å›²ã«ã‚¯ãƒ©ãƒ³ãƒ—
        targets = torch.clamp(targets, 6.0, 10.0)
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {features.shape} -> {targets.shape}")
        return features, targets
    
    def get_dataloader(self, batch_size: int = 32, train_ratio: float = 0.8):
        """
        ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’å–å¾—
        
        Args:
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            train_ratio: è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ¯”ç‡
        
        Returns:
            (train_loader, test_loader): è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        """
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        train_size = int(self.num_samples * train_ratio)
        test_size = self.num_samples - train_size
        
        train_features = self.features[:train_size]
        train_targets = self.targets[:train_size]
        test_features = self.features[train_size:]
        test_targets = self.targets[train_size:]
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        train_dataset = torch.utils.data.TensorDataset(train_features, train_targets)
        test_dataset = torch.utils.data.TensorDataset(test_features, test_targets)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
        train_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True
        )
        test_loader = torch.utils.data.DataLoader(
            test_dataset, batch_size=batch_size, shuffle=False
        )
        
        return train_loader, test_loader


class CNSTargetPredictor:
    """
    CNSã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬å™¨
    PWA+PET Transformerã‚’ä½¿ç”¨ã—ãŸå¤šã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬
    """
    
    def __init__(self, device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        self.device = device
        self.cns_targets = [
            '5HT2A', '5HT1A', 'D1', 'D2', 'CB1', 'CB2',
            'MOR', 'DOR', 'KOR', 'NOP', 'SERT', 'DAT', 'NET'
        ]
        self.chembl_targets = ChEMBLTargets()
    
    def create_model(self, use_pwa_pet: bool = True) -> nn.Module:
        """
        äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        
        Args:
            use_pwa_pet: PWA+PET Transformerã‚’ä½¿ç”¨ã™ã‚‹ã‹
        
        Returns:
            ãƒ¢ãƒ‡ãƒ«
        """
        if use_pwa_pet:
            model = TransformerRegressor(
                input_dim=2279,
                hidden_dim=512,
                num_layers=6,
                num_heads=8,
                num_targets=13,
                use_pwa_pet=True,
                buckets={"trivial": 2, "fund": 4, "adj": 2},
                use_rope=True,
                use_pet=True,
                pet_curv_reg=1e-5,
                dropout=0.1
            )
        else:
            model = TransformerRegressor(
                input_dim=2279,
                hidden_dim=512,
                num_layers=6,
                num_heads=8,
                num_targets=13,
                use_pwa_pet=False,
                dropout=0.1
            )
        
        return model.to(self.device)
    
    def train_model(
        self,
        model: nn.Module,
        train_loader: torch.utils.data.DataLoader,
        test_loader: torch.utils.data.DataLoader,
        num_epochs: int = 20,
        learning_rate: float = 1e-4
    ) -> Dict:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
        
        Args:
            model: ãƒ¢ãƒ‡ãƒ«
            train_loader: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
            test_loader: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
            num_epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            learning_rate: å­¦ç¿’ç‡
        
        Returns:
            è¨“ç·´çµæœ
        """
        criterion = nn.MSELoss()
        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
        
        train_losses = []
        test_losses = []
        train_times = []
        
        print(f"\nğŸš€ ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹... ({num_epochs} epochs)")
        
        for epoch in range(num_epochs):
            epoch_start = time.time()
            
            # è¨“ç·´
            model.train()
            train_loss = 0.0
            for batch_idx, (features, targets) in enumerate(train_loader):
                features, targets = features.to(self.device), targets.to(self.device)
                
                optimizer.zero_grad()
                
                if hasattr(model, 'use_pwa_pet') and model.use_pwa_pet:
                    outputs, reg_loss = model(features)
                    loss = criterion(outputs, targets) + reg_loss
                else:
                    outputs = model(features)
                    loss = criterion(outputs, targets)
                
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
            
            # ãƒ†ã‚¹ãƒˆ
            model.eval()
            test_loss = 0.0
            with torch.no_grad():
                for features, targets in test_loader:
                    features, targets = features.to(self.device), targets.to(self.device)
                    
                    if hasattr(model, 'use_pwa_pet') and model.use_pwa_pet:
                        outputs, _ = model(features)
                    else:
                        outputs = model(features)
                    
                    loss = criterion(outputs, targets)
                    test_loss += loss.item()
            
            epoch_time = time.time() - epoch_start
            avg_train_loss = train_loss / len(train_loader)
            avg_test_loss = test_loss / len(test_loader)
            
            train_losses.append(avg_train_loss)
            test_losses.append(avg_test_loss)
            train_times.append(epoch_time)
            
            scheduler.step()
            
            if (epoch + 1) % 5 == 0:
                print(f"  Epoch {epoch+1}/{num_epochs}: "
                      f"Train Loss={avg_train_loss:.4f}, "
                      f"Test Loss={avg_test_loss:.4f}, "
                      f"Time={epoch_time:.2f}s")
        
        return {
            "train_losses": train_losses,
            "test_losses": test_losses,
            "train_times": train_times,
            "final_test_loss": test_losses[-1]
        }
    
    def evaluate_model(
        self,
        model: nn.Module,
        test_loader: torch.utils.data.DataLoader
    ) -> Dict:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡
        
        Args:
            model: ãƒ¢ãƒ‡ãƒ«
            test_loader: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        
        Returns:
            è©•ä¾¡çµæœ
        """
        model.eval()
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for features, targets in test_loader:
                features, targets = features.to(self.device), targets.to(self.device)
                
                if hasattr(model, 'use_pwa_pet') and model.use_pwa_pet:
                    outputs, _ = model(features)
                else:
                    outputs = model(features)
                
                all_predictions.append(outputs.cpu())
                all_targets.append(targets.cpu())
        
        predictions = torch.cat(all_predictions, dim=0)
        targets = torch.cat(all_targets, dim=0)
        
        # å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®æ€§èƒ½ã‚’è¨ˆç®—
        target_performance = {}
        for i, target in enumerate(self.cns_targets):
            pred = predictions[:, i]
            true = targets[:, i]
            
            mse = torch.mean((pred - true) ** 2).item()
            mae = torch.mean(torch.abs(pred - true)).item()
            r2 = 1 - torch.sum((pred - true) ** 2) / torch.sum((true - torch.mean(true)) ** 2)
            r2 = r2.item()
            
            target_performance[target] = {
                "mse": mse,
                "mae": mae,
                "r2": r2
            }
        
        return {
            "target_performance": target_performance,
            "predictions": predictions,
            "targets": targets
        }


def plot_training_curves(results: Dict, model_name: str):
    """
    è¨“ç·´æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    
    Args:
        results: è¨“ç·´çµæœ
        model_name: ãƒ¢ãƒ‡ãƒ«å
    """
    plt.figure(figsize=(12, 4))
    
    # æå¤±æ›²ç·š
    plt.subplot(1, 2, 1)
    plt.plot(results["train_losses"], label="Training Loss", marker='o')
    plt.plot(results["test_losses"], label="Test Loss", marker='s')
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title(f"{model_name} - Training Curves")
    plt.legend()
    plt.grid(True)
    
    # è¨“ç·´æ™‚é–“
    plt.subplot(1, 2, 2)
    plt.plot(results["train_times"], label="Epoch Time", marker='o', color='green')
    plt.xlabel("Epoch")
    plt.ylabel("Time (s)")
    plt.title(f"{model_name} - Training Time")
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(f'{model_name.lower().replace(" ", "_")}_training.png', dpi=300, bbox_inches='tight')
    plt.show()


def plot_target_performance(evaluation_results: Dict, model_name: str):
    """
    ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ¥æ€§èƒ½ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    
    Args:
        evaluation_results: è©•ä¾¡çµæœ
        model_name: ãƒ¢ãƒ‡ãƒ«å
    """
    target_performance = evaluation_results["target_performance"]
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    targets = list(target_performance.keys())
    r2_scores = [target_performance[t]["r2"] for t in targets]
    mae_scores = [target_performance[t]["mae"] for t in targets]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # RÂ²ã‚¹ã‚³ã‚¢
    bars1 = ax1.bar(targets, r2_scores, color='skyblue')
    ax1.set_ylabel("RÂ² Score")
    ax1.set_title(f"{model_name} - RÂ² Score by Target")
    ax1.set_xticklabels(targets, rotation=45, ha='right')
    
    for bar, score in zip(bars1, r2_scores):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{score:.3f}', ha='center', va='bottom')
    
    # MAEã‚¹ã‚³ã‚¢
    bars2 = ax2.bar(targets, mae_scores, color='lightcoral')
    ax2.set_ylabel("MAE")
    ax2.set_title(f"{model_name} - MAE by Target")
    ax2.set_xticklabels(targets, rotation=45, ha='right')
    
    for bar, score in zip(bars2, mae_scores):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{score:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(f'{model_name.lower().replace(" ", "_")}_performance.png', dpi=300, bbox_inches='tight')
    plt.show()


def plot_prediction_scatter(evaluation_results: Dict, model_name: str):
    """
    äºˆæ¸¬æ•£å¸ƒå›³ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    
    Args:
        evaluation_results: è©•ä¾¡çµæœ
        model_name: ãƒ¢ãƒ‡ãƒ«å
    """
    predictions = evaluation_results["predictions"]
    targets = evaluation_results["targets"]
    
    # å…¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®äºˆæ¸¬ vs å®Ÿéš›ã®å€¤ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    fig, axes = plt.subplots(3, 5, figsize=(20, 12))
    axes = axes.flatten()
    
    cns_targets = [
        '5HT2A', '5HT1A', 'D1', 'D2', 'CB1', 'CB2',
        'MOR', 'DOR', 'KOR', 'NOP', 'SERT', 'DAT', 'NET'
    ]
    
    for i, target in enumerate(cns_targets):
        ax = axes[i]
        pred = predictions[:, i].numpy()
        true = targets[:, i].numpy()
        
        ax.scatter(true, pred, alpha=0.6, s=20)
        ax.plot([6, 10], [6, 10], 'r--', alpha=0.8)
        ax.set_xlabel(f"True {target} pIC50")
        ax.set_ylabel(f"Predicted {target} pIC50")
        ax.set_title(f"{target}")
        ax.grid(True, alpha=0.3)
        
        # RÂ²ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º
        r2 = 1 - np.sum((pred - true) ** 2) / np.sum((true - np.mean(true)) ** 2)
        ax.text(0.05, 0.95, f"RÂ² = {r2:.3f}", transform=ax.transAxes, 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
    
    # ä½™åˆ†ãªã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º
    for i in range(len(cns_targets), len(axes)):
        axes[i].set_visible(False)
    
    plt.suptitle(f"{model_name} - Prediction vs True Values", fontsize=16)
    plt.tight_layout()
    plt.savefig(f'{model_name.lower().replace(" ", "_")}_scatter.png', dpi=300, bbox_inches='tight')
    plt.show()


def print_performance_summary(evaluation_results: Dict, model_name: str):
    """
    æ€§èƒ½è¦ç´„ã‚’è¡¨ç¤º
    
    Args:
        evaluation_results: è©•ä¾¡çµæœ
        model_name: ãƒ¢ãƒ‡ãƒ«å
    """
    target_performance = evaluation_results["target_performance"]
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š {model_name} æ€§èƒ½è¦ç´„")
    print(f"{'='*60}")
    
    # å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®æ€§èƒ½
    print(f"\nğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ¥æ€§èƒ½:")
    print(f"{'Target':<8} {'RÂ²':<8} {'MAE':<8} {'MSE':<8}")
    print(f"{'-'*32}")
    
    for target, perf in target_performance.items():
        print(f"{target:<8} {perf['r2']:<8.3f} {perf['mae']:<8.3f} {perf['mse']:<8.3f}")
    
    # å¹³å‡æ€§èƒ½
    avg_r2 = np.mean([perf["r2"] for perf in target_performance.values()])
    avg_mae = np.mean([perf["mae"] for perf in target_performance.values()])
    avg_mse = np.mean([perf["mse"] for perf in target_performance.values()])
    
    print(f"\nğŸ“ˆ å¹³å‡æ€§èƒ½:")
    print(f"  RÂ² Score: {avg_r2:.3f}")
    print(f"  MAE: {avg_mae:.3f}")
    print(f"  MSE: {avg_mse:.3f}")


def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    print("ğŸ§¬ ChemForge CNSã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("="*60)
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸  ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    dataset = CNSMolecularDataset(num_samples=2000)
    train_loader, test_loader = dataset.get_dataloader(batch_size=32)
    
    # 2. äºˆæ¸¬å™¨ä½œæˆ
    predictor = CNSTargetPredictor(device)
    
    # 3. ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
    models_to_test = [
        ("Vanilla Transformer", False),
        ("PWA+PET Transformer", True)
    ]
    
    results = {}
    
    for model_name, use_pwa_pet in models_to_test:
        print(f"\nğŸ”¬ {model_name} ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        model = predictor.create_model(use_pwa_pet=use_pwa_pet)
        
        # è¨“ç·´
        training_results = predictor.train_model(
            model, train_loader, test_loader, num_epochs=15
        )
        
        # è©•ä¾¡
        evaluation_results = predictor.evaluate_model(model, test_loader)
        
        # çµæœä¿å­˜
        results[model_name] = {
            "training": training_results,
            "evaluation": evaluation_results
        }
        
        # å¯è¦–åŒ–
        plot_training_curves(training_results, model_name)
        plot_target_performance(evaluation_results, model_name)
        plot_prediction_scatter(evaluation_results, model_name)
        
        # æ€§èƒ½è¦ç´„
        print_performance_summary(evaluation_results, model_name)
    
    # 4. æ¯”è¼ƒè¦ç´„
    print(f"\n{'='*60}")
    print("ğŸ† ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒè¦ç´„")
    print(f"{'='*60}")
    
    for model_name, result in results.items():
        eval_result = result["evaluation"]
        avg_r2 = np.mean([perf["r2"] for perf in eval_result["target_performance"].values()])
        avg_mae = np.mean([perf["mae"] for perf in eval_result["target_performance"].values()])
        
        print(f"\nğŸ”¹ {model_name}")
        print(f"  å¹³å‡RÂ²: {avg_r2:.3f}")
        print(f"  å¹³å‡MAE: {avg_mae:.3f}")
        print(f"  æœ€çµ‚æå¤±: {result['training']['final_test_loss']:.4f}")
    
    print("\nğŸ‰ CNSã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ï¼")
    print("PWA+PET TransformeræŠ€è¡“ãŒCNSå‰µè–¬ã«æˆåŠŸè£ã«é©ç”¨ã•ã‚Œã¾ã—ãŸï¼")


if __name__ == "__main__":
    main()
