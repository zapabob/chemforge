"""
GNNå›å¸°ãƒ¢ãƒ‡ãƒ«

CNSå‰µè–¬å‘ã‘ã®Graph Neural Networkå›å¸°ãƒ¢ãƒ‡ãƒ«ã€‚
åˆ†å­ã‚°ãƒ©ãƒ•ã¨éª¨æ ¼ç‰¹å¾´é‡ã‚’çµ±åˆã—ã¦ã€13å€‹ã®CNSã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®pIC50ã‚’äºˆæ¸¬ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
import math


class GraphConvolution(nn.Module):
    """ã‚°ãƒ©ãƒ•ç•³ã¿è¾¼ã¿å±¤"""
    
    def __init__(self, in_features: int, out_features: int, bias: bool = True):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        
        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))
        if bias:
            self.bias = nn.Parameter(torch.FloatTensor(out_features))
        else:
            self.register_parameter('bias', None)
        
        self.reset_parameters()
    
    def reset_parameters(self):
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–"""
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)
    
    def forward(self, x: torch.Tensor, adj: torch.Tensor) -> torch.Tensor:
        """
        é †ä¼æ’­
        
        Args:
            x: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ (batch_size, num_nodes, in_features)
            adj: éš£æ¥è¡Œåˆ— (batch_size, num_nodes, num_nodes)
            
        Returns:
            å‡ºåŠ›ç‰¹å¾´é‡ (batch_size, num_nodes, out_features)
        """
        # ç·šå½¢å¤‰æ›
        support = torch.matmul(x, self.weight)
        
        # ã‚°ãƒ©ãƒ•ç•³ã¿è¾¼ã¿
        output = torch.matmul(adj, support)
        
        if self.bias is not None:
            output = output + self.bias
        
        return output


class GraphAttentionLayer(nn.Module):
    """ã‚°ãƒ©ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤"""
    
    def __init__(self, in_features: int, out_features: int, dropout: float = 0.1, alpha: float = 0.2):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.dropout = dropout
        self.alpha = alpha
        
        self.W = nn.Linear(in_features, out_features, bias=False)
        self.a = nn.Linear(2 * out_features, 1, bias=False)
        
        self.leakyrelu = nn.LeakyReLU(alpha)
        self.dropout_layer = nn.Dropout(dropout)
        
        self.reset_parameters()
    
    def reset_parameters(self):
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–"""
        nn.init.xavier_uniform_(self.W.weight)
        nn.init.xavier_uniform_(self.a.weight)
    
    def forward(self, x: torch.Tensor, adj: torch.Tensor) -> torch.Tensor:
        """
        é †ä¼æ’­
        
        Args:
            x: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ (batch_size, num_nodes, in_features)
            adj: éš£æ¥è¡Œåˆ— (batch_size, num_nodes, num_nodes)
            
        Returns:
            å‡ºåŠ›ç‰¹å¾´é‡ (batch_size, num_nodes, out_features)
        """
        batch_size, num_nodes, _ = x.size()
        
        # ç·šå½¢å¤‰æ›
        h = self.W(x)  # (batch_size, num_nodes, out_features)
        
        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—
        a_input = self._prepare_attentional_mechanism_input(h)
        e = self.leakyrelu(self.a(a_input))  # (batch_size, num_nodes, num_nodes)
        
        # éš£æ¥è¡Œåˆ—ã§ãƒã‚¹ã‚¯
        zero_vec = -9e15 * torch.ones_like(e)
        attention = torch.where(adj > 0, e, zero_vec)
        attention = F.softmax(attention, dim=-1)
        attention = self.dropout_layer(attention)
        
        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã‚’é©ç”¨
        h_prime = torch.matmul(attention, h)
        
        return h_prime
    
    def _prepare_attentional_mechanism_input(self, h: torch.Tensor) -> torch.Tensor:
        """ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã®å…¥åŠ›ã‚’æº–å‚™"""
        batch_size, num_nodes, out_features = h.size()
        
        # å…¨ãƒãƒ¼ãƒ‰ãƒšã‚¢ã®ç‰¹å¾´é‡ã‚’çµåˆ
        h_repeated_in_chunks = h.repeat_interleave(num_nodes, dim=1)
        h_repeated_alternating = h.repeat(1, num_nodes, 1)
        
        all_combinations_matrix = torch.cat([
            h_repeated_in_chunks, h_repeated_alternating
        ], dim=-1)
        
        return all_combinations_matrix.view(batch_size, num_nodes, num_nodes, 2 * out_features)


class GNNRegressor(nn.Module):
    """GNNå›å¸°ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(
        self,
        node_features: int = 78,
        edge_features: int = 12,
        hidden_dim: int = 256,
        num_layers: int = 3,
        num_targets: int = 13,
        dropout: float = 0.1,
        use_attention: bool = True,
        use_scaffold_features: bool = True,
        scaffold_dim: int = 20
    ):
        """
        GNNå›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
        
        Args:
            node_features: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡æ¬¡å…ƒ
            edge_features: ã‚¨ãƒƒã‚¸ç‰¹å¾´é‡æ¬¡å…ƒ
            hidden_dim: éš ã‚Œå±¤æ¬¡å…ƒ
            num_layers: GNNå±¤æ•°
            num_targets: äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ•°
            dropout: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
            use_attention: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            use_scaffold_features: éª¨æ ¼ç‰¹å¾´é‡ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            scaffold_dim: éª¨æ ¼ç‰¹å¾´é‡æ¬¡å…ƒ
        """
        super().__init__()
        
        self.node_features = node_features
        self.edge_features = edge_features
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.num_targets = num_targets
        self.use_attention = use_attention
        self.use_scaffold_features = use_scaffold_features
        self.scaffold_dim = scaffold_dim
        
        # ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡æŠ•å½±
        self.node_projection = nn.Linear(node_features, hidden_dim)
        
        # ã‚¨ãƒƒã‚¸ç‰¹å¾´é‡æŠ•å½±
        self.edge_projection = nn.Linear(edge_features, hidden_dim)
        
        # GNNå±¤
        self.gnn_layers = nn.ModuleList()
        for i in range(num_layers):
            if use_attention:
                layer = GraphAttentionLayer(hidden_dim, hidden_dim, dropout)
            else:
                layer = GraphConvolution(hidden_dim, hidden_dim)
            self.gnn_layers.append(layer)
        
        # éª¨æ ¼ç‰¹å¾´é‡æŠ•å½±
        if use_scaffold_features:
            self.scaffold_projection = nn.Linear(scaffold_dim, hidden_dim)
        
        # ã‚°ãƒ©ãƒ•ãƒ—ãƒ¼ãƒªãƒ³ã‚°
        self.graph_pooling = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # å‡ºåŠ›å±¤
        if use_scaffold_features:
            output_dim = hidden_dim // 4 + hidden_dim // 4  # ã‚°ãƒ©ãƒ•ç‰¹å¾´é‡ + éª¨æ ¼ç‰¹å¾´é‡
        else:
            output_dim = hidden_dim // 4
        
        self.output_layer = nn.Sequential(
            nn.Linear(output_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_targets)
        )
        
        # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
        self.dropout = nn.Dropout(dropout)
        
        # é‡ã¿åˆæœŸåŒ–
        self._init_weights()
    
    def _init_weights(self):
        """é‡ã¿ã‚’åˆæœŸåŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(
        self,
        node_features: torch.Tensor,
        edge_features: torch.Tensor,
        adj_matrix: torch.Tensor,
        scaffold_features: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        é †ä¼æ’­
        
        Args:
            node_features: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ (batch_size, num_nodes, node_features)
            edge_features: ã‚¨ãƒƒã‚¸ç‰¹å¾´é‡ (batch_size, num_nodes, num_nodes, edge_features)
            adj_matrix: éš£æ¥è¡Œåˆ— (batch_size, num_nodes, num_nodes)
            scaffold_features: éª¨æ ¼ç‰¹å¾´é‡ (batch_size, scaffold_dim)
            
        Returns:
            äºˆæ¸¬å€¤ (batch_size, num_targets)
        """
        batch_size, num_nodes, _ = node_features.size()
        
        # ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã‚’æŠ•å½±
        x = self.node_projection(node_features)  # (batch_size, num_nodes, hidden_dim)
        
        # GNNå±¤ã‚’é©ç”¨
        for layer in self.gnn_layers:
            if self.use_attention:
                x = layer(x, adj_matrix)
            else:
                x = layer(x, adj_matrix)
            x = F.relu(x)
            x = self.dropout(x)
        
        # ã‚°ãƒ©ãƒ•ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼ˆå¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼‰
        graph_features = x.mean(dim=1)  # (batch_size, hidden_dim)
        graph_features = self.graph_pooling(graph_features)  # (batch_size, hidden_dim // 4)
        
        # éª¨æ ¼ç‰¹å¾´é‡ã‚’çµ±åˆ
        if self.use_scaffold_features and scaffold_features is not None:
            scaffold_proj = self.scaffold_projection(scaffold_features)  # (batch_size, hidden_dim)
            scaffold_pooled = self.graph_pooling(scaffold_proj)  # (batch_size, hidden_dim // 4)
            
            # ç‰¹å¾´é‡ã‚’çµåˆ
            combined_features = torch.cat([graph_features, scaffold_pooled], dim=-1)
        else:
            combined_features = graph_features
        
        # å‡ºåŠ›å±¤
        output = self.output_layer(combined_features)  # (batch_size, num_targets)
        
        return output
    
    def get_node_embeddings(
        self,
        node_features: torch.Tensor,
        edge_features: torch.Tensor,
        adj_matrix: torch.Tensor
    ) -> torch.Tensor:
        """
        ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
        
        Args:
            node_features: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡
            edge_features: ã‚¨ãƒƒã‚¸ç‰¹å¾´é‡
            adj_matrix: éš£æ¥è¡Œåˆ—
            
        Returns:
            ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ (batch_size, num_nodes, hidden_dim)
        """
        batch_size, num_nodes, _ = node_features.size()
        
        # ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã‚’æŠ•å½±
        x = self.node_projection(node_features)
        
        # GNNå±¤ã‚’é©ç”¨
        for layer in self.gnn_layers:
            if self.use_attention:
                x = layer(x, adj_matrix)
            else:
                x = layer(x, adj_matrix)
            x = F.relu(x)
            x = self.dropout(x)
        
        return x
    
    def get_graph_embeddings(
        self,
        node_features: torch.Tensor,
        edge_features: torch.Tensor,
        adj_matrix: torch.Tensor,
        scaffold_features: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        ã‚°ãƒ©ãƒ•åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
        
        Args:
            node_features: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡
            edge_features: ã‚¨ãƒƒã‚¸ç‰¹å¾´é‡
            adj_matrix: éš£æ¥è¡Œåˆ—
            scaffold_features: éª¨æ ¼ç‰¹å¾´é‡
            
        Returns:
            ã‚°ãƒ©ãƒ•åŸ‹ã‚è¾¼ã¿
        """
        batch_size, num_nodes, _ = node_features.size()
        
        # ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã‚’æŠ•å½±
        x = self.node_projection(node_features)
        
        # GNNå±¤ã‚’é©ç”¨
        for layer in self.gnn_layers:
            if self.use_attention:
                x = layer(x, adj_matrix)
            else:
                x = layer(x, adj_matrix)
            x = F.relu(x)
            x = self.dropout(x)
        
        # ã‚°ãƒ©ãƒ•ãƒ—ãƒ¼ãƒªãƒ³ã‚°
        graph_features = x.mean(dim=1)
        graph_features = self.graph_pooling(graph_features)
        
        # éª¨æ ¼ç‰¹å¾´é‡ã‚’çµ±åˆ
        if self.use_scaffold_features and scaffold_features is not None:
            scaffold_proj = self.scaffold_projection(scaffold_features)
            scaffold_pooled = self.graph_pooling(scaffold_proj)
            combined_features = torch.cat([graph_features, scaffold_pooled], dim=-1)
        else:
            combined_features = graph_features
        
        return combined_features
    
    def predict_with_uncertainty(
        self,
        node_features: torch.Tensor,
        edge_features: torch.Tensor,
        adj_matrix: torch.Tensor,
        scaffold_features: Optional[torch.Tensor] = None,
        num_samples: int = 100
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã—ãŸäºˆæ¸¬
        
        Args:
            node_features: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡
            edge_features: ã‚¨ãƒƒã‚¸ç‰¹å¾´é‡
            adj_matrix: éš£æ¥è¡Œåˆ—
            scaffold_features: éª¨æ ¼ç‰¹å¾´é‡
            num_samples: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å›æ•°
            
        Returns:
            äºˆæ¸¬å€¤ã¨ä¸ç¢ºå®Ÿæ€§
        """
        self.eval()
        predictions = []
        
        with torch.no_grad():
            for _ in range(num_samples):
                # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’æœ‰åŠ¹ã«ã—ã¦äºˆæ¸¬
                self.train()
                pred = self.forward(node_features, edge_features, adj_matrix, scaffold_features)
                predictions.append(pred)
                self.eval()
        
        predictions = torch.stack(predictions)  # (num_samples, batch_size, num_targets)
        
        # å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—
        mean_pred = predictions.mean(dim=0)
        std_pred = predictions.std(dim=0)
        
        return mean_pred, std_pred
    
    def save_model(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        torch.save({
            'model_state_dict': self.state_dict(),
            'node_features': self.node_features,
            'edge_features': self.edge_features,
            'hidden_dim': self.hidden_dim,
            'num_layers': self.num_layers,
            'num_targets': self.num_targets,
            'use_attention': self.use_attention,
            'use_scaffold_features': self.use_scaffold_features,
            'scaffold_dim': self.scaffold_dim
        }, path)
    
    @classmethod
    def load_model(cls, path: str, device: str = 'cpu'):
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        checkpoint = torch.load(path, map_location=device)
        
        model = cls(
            node_features=checkpoint['node_features'],
            edge_features=checkpoint['edge_features'],
            hidden_dim=checkpoint['hidden_dim'],
            num_layers=checkpoint['num_layers'],
            num_targets=checkpoint['num_targets'],
            use_attention=checkpoint['use_attention'],
            use_scaffold_features=checkpoint['use_scaffold_features'],
            scaffold_dim=checkpoint['scaffold_dim']
        )
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)
        
        return model


# ä¾¿åˆ©é–¢æ•°
def create_gnn_model(
    node_features: int = 78,
    edge_features: int = 12,
    hidden_dim: int = 256,
    num_layers: int = 3,
    num_targets: int = 13,
    use_attention: bool = True,
    use_scaffold_features: bool = True
) -> GNNRegressor:
    """GNNãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
    return GNNRegressor(
        node_features=node_features,
        edge_features=edge_features,
        hidden_dim=hidden_dim,
        num_layers=num_layers,
        num_targets=num_targets,
        use_attention=use_attention,
        use_scaffold_features=use_scaffold_features
    )


def count_parameters(model: nn.Module) -> int:
    """ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¨ˆç®—"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    print("ğŸ§¬ GNNå›å¸°ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    model = create_gnn_model(
        node_features=78,
        edge_features=12,
        hidden_dim=256,
        num_layers=3,
        num_targets=13,
        use_attention=True,
        use_scaffold_features=True
    )
    
    print(f"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {count_parameters(model):,}")
    
    # ãƒ†ã‚¹ãƒˆç”¨å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
    batch_size = 32
    num_nodes = 50
    node_features = 78
    edge_features = 12
    scaffold_dim = 20
    
    # å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ
    node_feat = torch.randn(batch_size, num_nodes, node_features)
    edge_feat = torch.randn(batch_size, num_nodes, num_nodes, edge_features)
    adj_matrix = torch.randint(0, 2, (batch_size, num_nodes, num_nodes)).float()
    scaffold_feat = torch.randn(batch_size, scaffold_dim)
    
    print(f"ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡å½¢çŠ¶: {node_feat.shape}")
    print(f"ã‚¨ãƒƒã‚¸ç‰¹å¾´é‡å½¢çŠ¶: {edge_feat.shape}")
    print(f"éš£æ¥è¡Œåˆ—å½¢çŠ¶: {adj_matrix.shape}")
    print(f"éª¨æ ¼ç‰¹å¾´é‡å½¢çŠ¶: {scaffold_feat.shape}")
    
    # é †ä¼æ’­
    model.eval()
    with torch.no_grad():
        output = model(node_feat, edge_feat, adj_matrix, scaffold_feat)
        print(f"å‡ºåŠ›å½¢çŠ¶: {output.shape}")
        
        # ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
        node_embeddings = model.get_node_embeddings(node_feat, edge_feat, adj_matrix)
        print(f"ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿å½¢çŠ¶: {node_embeddings.shape}")
        
        # ã‚°ãƒ©ãƒ•åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
        graph_embeddings = model.get_graph_embeddings(node_feat, edge_feat, adj_matrix, scaffold_feat)
        print(f"ã‚°ãƒ©ãƒ•åŸ‹ã‚è¾¼ã¿å½¢çŠ¶: {graph_embeddings.shape}")
    
    # ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã—ãŸäºˆæ¸¬
    print("\nä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã—ãŸäºˆæ¸¬:")
    mean_pred, std_pred = model.predict_with_uncertainty(
        node_feat, edge_feat, adj_matrix, scaffold_feat, num_samples=10
    )
    print(f"å¹³å‡äºˆæ¸¬å½¢çŠ¶: {mean_pred.shape}")
    print(f"æ¨™æº–åå·®å½¢çŠ¶: {std_pred.shape}")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
    print("\nãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ:")
    model_path = "test_gnn_model.pth"
    model.save_model(model_path)
    
    loaded_model = GNNRegressor.load_model(model_path)
    print(f"èª­ã¿è¾¼ã¿æˆåŠŸ: {type(loaded_model)}")
    
    # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    import os
    if os.path.exists(model_path):
        os.remove(model_path)
    
    print("\nâœ… GNNå›å¸°ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
